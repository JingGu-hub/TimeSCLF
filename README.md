# TimeSCLF: A Semi-supervised Contrastive Learning Framework for Time Series Open-set Noisy labels

The performance of deep neural networks suffers significantly when trained on data containing label noise, where labels are incorrect. In practical scenarios, label noise primarily manifests as either closed-set or open-set. Closed-set noise occurs when samples are misassigned to an existing class within the dataset. In contrast, open-set noise refers to samples that do not belong to any predefined class in the dataset. While notable progress addresses open-set label noise in computer vision, time series data analysis still encounters unique challenges in this domain.
Firstly, existing methods typically categorize samples into clean, closed-set noisy, and open-set noisy. However, conventional approaches often rely on predetermined thresholds for this categorization, which hinders accurate differentiation among these three sample types in novel environments. Consequently, developing an adaptive sample screening mechanism that precisely distinguishes these categories remains a significant research challenge. Secondly, although contrastive learning has been adopted in the noisy label field, its application to effectively leverage diverse views of time series data for enhanced noise resistance is still underexplored.
To address these challenges, this paper introduces a semi-supervised contrastive learning framework for time series data (TimeSCLF), designed to mitigate the open-set label noise problem effectively. Specifically, TimeSCLF incorporates a novel two-stage adaptive sample selection strategy (TASS). The first stage prioritizes identifying and filtering high-confidence clean samples. In the second stage, TASS leverages the inherent characteristic that open-set samples do not belong to any established class, assigning labels to accurately differentiate between closed-set and open-set noise.
Furthermore, to bolster the model's robustness against label noise interference, we integrate Multi-view Temporal Contrastive Learning (MvTCL). This method improves the representation capability within a single view of time series data while simultaneously increasing the representational divergence between different views. This approach fully exploits the complementary information across views. For instance, if one view (e.g., a size downsampled view) exhibits low confidence in predicting noisy labeled samples, the confidence predictions from other views provide valuable supplementary information, thereby substantially enhancing the model's resilience to label noise.
Extensive experiments on various standard time series datasets, including those with mixed open-set and closed-set noise, demonstrate that TimeSCLF significantly outperforms current state-of-the-art methods. 
